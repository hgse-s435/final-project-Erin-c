{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a folder for all text files\n",
    "\n",
    "path = '/Users/erincarvalho/Desktop/dev/final-project-Erin-c'\n",
    "if os.path.isdir(path + '/txt_files'):\n",
    "    shutil.rmtree(path + '/txt_files', ignore_errors=False, onerror=None)\n",
    "os.mkdir(path + '/txt_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a separate text file for each topic with all posts and replies from csv\n",
    "# ScratchEd_all_data.csv\n",
    "\n",
    "ids = []\n",
    "\n",
    "with open('ScratchEd_all_data.csv', \"r\", encoding='utf-8', errors='ignore') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    count = 0\n",
    "\n",
    "    for idx, row in enumerate(csv_reader):   \n",
    "        if str(row[0]) in ids:\n",
    "            filename = path + '/txt_files/topic_' + str(row[0]) + '.txt'\n",
    "            file = open(filename,'a+')\n",
    "            contents = str(row[3]) + '\\r\\n' + '\\r\\n'\n",
    "            file.write(contents)\n",
    "        else:\n",
    "            filename = path + '/txt_files/topic_' + str(row[0]) + '.txt'\n",
    "            file = open(filename,'a+')\n",
    "            contents = str(row[3]) + '\\r\\n' + '\\r\\n'\n",
    "            file.write(contents)\n",
    "            ids.append(str(row[0]))\n",
    "            count += 1\n",
    "    print(count)\n",
    "    print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# save all the text files in a list\n",
    "\n",
    "threads = glob.glob('./txt_files/*.txt')\n",
    "print(len(threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "# load actual text into a list\n",
    "\n",
    "for thread in threads: \n",
    "    with open (thread, \"r\", encoding='utf-8', errors='ignore') as t:\n",
    "        documents.append(t.read())\n",
    "        \n",
    "# convert text to all lowercase\n",
    "\n",
    "for i, t in enumerate(threads):\n",
    "    documents[i] = documents[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "              'very', 's', 't', 'can', 'will', 'just', 'don', 'should', \n",
    "              'now', 'http', 'https', 'edu', 'www', 'com', 'scratch', \n",
    "              'mit', 'org', 'would', 'should', 'could', 'might', 'really', \n",
    "              'very', 'good', 'great', 'best', 'karen', '྾explore',\n",
    "              '྾interact', '྾network', 'get', 'also', 'let', 'much', 'use', \n",
    "              'les', 'ver', 'post', 'est', 'oscar', 'con', 'las', 'para',\n",
    "              'student', 'projects', 'january', 'february', 'march', 'april',\n",
    "              'may', 'june', 'july', 'august', 'september', 'october', \n",
    "              'november','december']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list_of_documents(documents):\n",
    "    '''cleans a list of documents'''\n",
    "    \n",
    "    cleaned_docs = []\n",
    "    \n",
    "    for i,doc in enumerate(documents):\n",
    "        # removes new lines and carriage returns\n",
    "        doc = doc.replace('\\n', ' ')\n",
    "        doc = doc.replace('\\r', ' ')\n",
    "        # remove ponctuation\n",
    "        for punc in punctuation: \n",
    "            doc = doc.replace(punc, ' ')\n",
    "        # remove numbers\n",
    "        for i in range(10):\n",
    "            doc = doc.replace(str(i), ' ')\n",
    "        # remove stop words\n",
    "        for stop_word in stop_words:\n",
    "            doc = doc.replace(' ' + stop_word + ' ', ' ')\n",
    "        # remove single characters and stem the words \n",
    "        doc = [x for x in doc.split() if len(x) > 2]\n",
    "        doc = \" \".join(doc)\n",
    "        # save the result to our list of documents\n",
    "        cleaned_docs.append(doc)\n",
    "        \n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first bit of the document for sanity\n",
    "\n",
    "clean_docs = clean_list_of_documents(documents)\n",
    "\n",
    "print(clean_docs[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nltk\n",
    "# !nltk.download(\"wordnet\", \"./\")\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(documents):\n",
    "    '''builds a vocabulary'''\n",
    "\n",
    "    lemmatized_vocabulary = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "\n",
    "    for document in clean_docs:\n",
    "        tokens = word_tokenize(document)\n",
    "        for word, tag in pos_tag(tokens):\n",
    "            word = lemmatizer.lemmatize(word, tag_map[tag[0]])\n",
    "            if wn.synsets(word):\n",
    "                if word not in lemmatized_vocabulary: \n",
    "                    lemmatized_vocabulary.append(word)\n",
    "\n",
    "    lemmatized_vocabulary = list(set(lemmatized_vocabulary))\n",
    "    lemmatized_vocabulary.sort()\n",
    "\n",
    "    return lemmatized_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(clean_docs)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_overlap(documents, window_size=100, overlap=25):\n",
    "    \n",
    "    # create the list of overlapping documents\n",
    "    new_list_of_documents = []\n",
    "    \n",
    "    # flatten everything into one string\n",
    "    flat = \"\"\n",
    "    for document in documents:\n",
    "        flat += document\n",
    "    \n",
    "    # split into words\n",
    "    flat = flat.split()\n",
    "\n",
    "    # create chunks of 100 words\n",
    "    high = window_size\n",
    "    while high < len(flat):\n",
    "        low = high - window_size\n",
    "        new_list_of_documents.append(flat[low:high])\n",
    "        high += overlap\n",
    "    return new_list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = flatten_and_overlap(clean_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(0, index=np.arange(len(chunks)), columns=vocabulary)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_by_words_df(chunks, vocabulary):\n",
    "    df = pd.DataFrame(0, index=np.arange(len(chunks)), columns=vocabulary)\n",
    "    \n",
    "    # fill out the matrix with counts\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        for word in chunk:\n",
    "            if word in df.columns: \n",
    "                df.loc[i,word] += 1\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[0])\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = docs_by_words_df(chunks, vocabulary)\n",
    "df.loc[0,'school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_plus_log(cell):\n",
    "    if cell != 0: \n",
    "        return 1 + math.log(cell)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df.applymap(one_plus_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_plus_log_mat(df):\n",
    "    df = df.applymap(one_plus_log)\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before one + log: \", df.loc[0,'school'])\n",
    "print(\"after one + log: \", 1 + math.log(df.loc[0,'school']))\n",
    "print(\"Value in the dataframe: \", df_log.loc[0,'school'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "df_log[df_log.columns] = scaler.fit_transform(df_log[df_log.columns])\n",
    "df_log[df_log.columns[100:600]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "def normalize_df(df, method='Normalizer'):\n",
    "    \n",
    "    # choose the normalization strategy\n",
    "    scaler = None\n",
    "    if method == 'Normalizer': scaler = Normalizer()\n",
    "    if method == 'MinMaxScaler': scaler = MinMaxScaler()\n",
    "    if method == 'StandardScaler': scaler = StandardScaler()\n",
    "        \n",
    "    # apply the normalization\n",
    "    if scaler != None:\n",
    "        df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "\n",
    "    # return the resulting dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_sum = np.sum(df_log.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    return np.sqrt(np.dot(u, u))\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / vector_length(u)\n",
    "\n",
    "v_avg = length_norm(v_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = df_log.values\n",
    "\n",
    "for row in range(df_log.shape[0]):\n",
    "\n",
    "    # this is one vector (row\n",
    "    v_i = matrix[row,:]\n",
    "\n",
    "    # we subtract its component along v_average\n",
    "    scalar = np.dot(v_i,v_avg)\n",
    "    sub = v_avg * scalar\n",
    "\n",
    "    # we replace the row by the deviation vector\n",
    "    matrix[row,:] = length_norm(v_i - sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    return np.sqrt(np.dot(u, u))\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / vector_length(u)\n",
    "\n",
    "def transform_deviation_vectors(df):\n",
    "    \n",
    "    # get the numpy matrix from the df\n",
    "    matrix = df.values\n",
    "    \n",
    "    # compute the sum of the vectors\n",
    "    v_sum = np.sum(matrix, axis=0)\n",
    "    \n",
    "    # normalize this vector (find its average)\n",
    "    v_avg = length_norm(v_sum)\n",
    "    \n",
    "    # we iterate through each vector\n",
    "    for row in range(df_log.shape[0]):\n",
    "        \n",
    "        # this is one vector (row\n",
    "        v_i = matrix[row,:]\n",
    "        \n",
    "        # we subtract its component along v_average\n",
    "        scalar = np.dot(v_i,v_avg)\n",
    "        sub = v_avg * scalar\n",
    "        \n",
    "        # we replace the row by the deviation vector\n",
    "        matrix[row,:] = length_norm(v_i - sub)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_deviation_vectors(df_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_obj = KMeans(n_clusters=10, max_iter=1000).fit(df.values)\n",
    "\n",
    "n_words = 10\n",
    "top_words = collections.defaultdict(lambda: [])\n",
    "\n",
    "# iterate through each cluster\n",
    "for n in range(kmeans_obj.n_clusters):\n",
    "\n",
    "    print('CLUSTER ' + str(n+1) + ': ', end='')\n",
    "\n",
    "    # get the cluster centers\n",
    "    arr = kmeans_obj.cluster_centers_[n]\n",
    "\n",
    "    # sorts the array and keep the last n words\n",
    "    indices = arr.argsort()[-n_words:]\n",
    "\n",
    "    # add the words to the list of words\n",
    "    for i in indices:\n",
    "        print(vocabulary[i], end=', ')\n",
    "        top_words[n].append(vocabulary[i])\n",
    "        \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ward = AgglomerativeClustering(n_clusters=10, linkage='ward').fit(df.values)\n",
    "label = ward.labels_\n",
    "\n",
    "print(\"Number of points: %i\" % label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "import numpy as np\n",
    "\n",
    "clf = NearestCentroid()\n",
    "clf.fit(df.values, label)\n",
    "\n",
    "print(clf.centroids_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(df, n_clusters, centroids, n_words=10, printed=True):   \n",
    "    # try to get the most informative words of each cluster\n",
    "    words = {}\n",
    "    vocabulary = df.columns\n",
    "    for n in range(n_clusters):\n",
    "        words[n] = []\n",
    "        if printed: print('CLUSTER ' + str(n+1) + ': ', end='')\n",
    "        arr = centroids[n]\n",
    "        indices = arr.argsort()[-n_words:]\n",
    "        for i in indices:\n",
    "            if printed: print(vocabulary[i], end=', '),\n",
    "            words[n].append(vocabulary[i])\n",
    "        print('')\n",
    "    return words\n",
    "\n",
    "top_words = visualize_clusters(df, clf.centroids_.shape[0], clf.centroids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "\n",
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance: model\n",
    "model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles\n",
    "model.fit(df.values)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(df.values)\n",
    "\n",
    "# Print the NMF features\n",
    "print(nmf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: components_df\n",
    "components_df = pd.DataFrame(model.components_, columns=df.columns)\n",
    "\n",
    "for i in range(6):\n",
    "\n",
    "    # Select row 3: component\n",
    "    component = components_df.iloc[i,:]\n",
    "\n",
    "    # Print result of nlargest\n",
    "    print(component.nlargest(n=10), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_top_words(kmeans, centers, n_words=10):\n",
    "    \n",
    "    top_words = collections.defaultdict(lambda: [])\n",
    "\n",
    "    # iterate through each cluster\n",
    "    for n in range(kmeans.n_clusters):\n",
    "\n",
    "        # get the cluster centers\n",
    "        arr = centers[n]\n",
    "\n",
    "        # sorts the array and keep the last n words\n",
    "        indices = arr.argsort()[-n_words:]\n",
    "\n",
    "        # add the words to the list of words\n",
    "        for i in indices:\n",
    "            top_words[n].append(vocabulary[i])\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_clusters = get_top_words(kmeans_obj, clf.centroids_)\n",
    "print(top_10_clusters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Category10\n",
    "\n",
    "colors = Category10[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "html_text = \"\"\n",
    "\n",
    "\n",
    "for i in range(0,kmeans_obj.n_clusters):\n",
    "    words=', '.join(top_words[i])\n",
    "    color = colors[i]\n",
    "    text = \"<p>Cluster X: <font color='\"+color+\"'>\"+words+\"</font></p>\"\n",
    "    html_text += text\n",
    "    \n",
    "HTML(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices =  list(range(0, len(chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_chunks = [' '.join(chunks[i]) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [kmeans_obj.labels_[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [colors[labels[i]] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = []\n",
    "current_doc = 0\n",
    "next_doc = 1\n",
    "\n",
    "# we go through all the chunks \n",
    "for chunk in list_of_chunks:\n",
    "    next_doc = current_doc + 1\n",
    "    if next_doc == len(clean_docs):\n",
    "        doc_id.append(current_doc)\n",
    "    else:\n",
    "        if chunk in clean_docs[next_doc]:\n",
    "            current_doc += 1\n",
    "        doc_id.append(current_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(indices))\n",
    "print(len(list_of_chunks))\n",
    "print(len(labels))\n",
    "print(len(doc_id))\n",
    "print(len(palette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = {'indices': indices,\n",
    "          'chunk': list_of_chunks, \n",
    "          'cluster': labels,\n",
    "          'document': doc_id, \n",
    "          'palette': palette }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame.from_dict(master)\n",
    "\n",
    "master_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import ColumnDataSource, figure, show, output_file\n",
    "from bokeh.io import output_notebook, curdoc\n",
    "from bokeh.models import HoverTool, Select, Slider\n",
    "from bokeh.layouts import row, column\n",
    "\n",
    "source = ColumnDataSource(master_df)\n",
    "\n",
    "# Create a figure with the \"box_select\" tool: p\n",
    "p = figure(tools='box_select',x_axis_label='indices',y_axis_label='document')\n",
    "\n",
    "# Add circle glyphs to the figure p\n",
    "p.circle('indices','document', source=source, color='green', size=8)\n",
    "\n",
    "# Specify the name of the output file and show the result\n",
    "output_file('output.html')\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(master_df)\n",
    "p = figure(tools='box_select',x_axis_label='indices',y_axis_label='document')\n",
    "p.circle('indices','cluster', source=source, color='palette', size=8)\n",
    "output_file('output.html')\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HoverTool: hover\n",
    "hover = HoverTool(tooltips=[('chunk', '@chunk')], mode='vline')\n",
    "\n",
    "# Add hover tool to p\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Show the new output with the hover tool\n",
    "output_file('output.html')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(results_clustering, top_words, vocabulary):\n",
    "    text = \"\"\n",
    "\n",
    "    for cluster, words in top_words.items(): \n",
    "        words = \" \".join(words)\n",
    "        color = colors[cluster]\n",
    "        text += \"<p>Cluster \"+str(cluster)+\": <font color='\"+color+\"'>\"+words+\"</font></p>\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    df = docs_by_words_df(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    df.values = one_plus_log_mat(df)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    df.values = normalize_df(df, method='Normalizer')\n",
    "    \n",
    "    # step 8: we compute deviatio vectors\n",
    "    df = transform_deviation_vectors(df)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = KMeans(n_clusters=numTopics, max_iter=1000).fit(df.values)\n",
    "    \n",
    "    # step 10: we get the top words for each cluster\n",
    "    top_words = get_top_words(results_clustering, results_clustering.cluster_centers_)\n",
    "    \n",
    "    # step 11: we create a visualization for the topics\n",
    "    visualization = visualize_clusters(results_clustering, top_words, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, df, top_words, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = {}\n",
    "keys = []\n",
    "values = []\n",
    "\n",
    "for thread in threads:\n",
    "    keys.append(thread[18:-4])\n",
    "\n",
    "for document in documents:\n",
    "    values.append(document)\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    posts[keys[i]] = values[i]\n",
    "\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_word(string, word):\n",
    "    return (' ' + word + ' ') in (' ' + string + ' ')\n",
    "\n",
    "query = input('What are you searching for? ')\n",
    "\n",
    "results = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for key,val in posts.items():\n",
    "    if contains_word(posts[key], query):\n",
    "        counter += 1\n",
    "        #print(val)\n",
    "        results.append(val)\n",
    "        # print(key, val)\n",
    "print('counter is ' + str(counter))\n",
    "\n",
    "clean_results = clean_list_of_documents(results)\n",
    "result_vocabulary = get_vocabulary(clean_results)\n",
    "\n",
    "print(clean_results, result_vocabulary)\n",
    "print(len(clean_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim\n",
    "from collections import defaultdict\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import itertools\n",
    "\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in clean_results]\n",
    "\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "query_id = dictionary.token2id.get(query)\n",
    "\n",
    "print(query_id)\n",
    "\n",
    "corpus = [dictionary.doc2bow(tokenized_doc) for tokenized_doc in tokenized_docs]\n",
    "\n",
    "total_word_count = defaultdict(int)\n",
    "\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "for c in range(len(corpus)):\n",
    "    doc = corpus[c]\n",
    "\n",
    "    # Calculate the tfidf weights of doc: tfidf_weights\n",
    "    tfidf_weights = tfidf[doc]\n",
    "\n",
    "    # Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # Print the top 5 weighted words\n",
    "    for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "        print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.layouts import row\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "\n",
    "freq_words = [dictionary.get(word_id) for word_id, word_count in sorted_word_count[:5]]\n",
    "freq_count =  [word_count for word_id, word_count in sorted_word_count[:5]]\n",
    "\n",
    "# print(freq_words, freq_count)\n",
    "\n",
    "dot = figure(title=\"Most Frequent Words\", tools=\"\", toolbar_location=None,\n",
    "            y_range=freq_words, x_range=[0,max(freq_count) + 10])\n",
    "\n",
    "dot.segment(0, freq_words, freq_count, freq_words, line_width=2, line_color=\"green\", )\n",
    "dot.circle(freq_count, freq_words, size=15, fill_color=\"orange\", line_color=\"green\", line_width=3, )\n",
    "\n",
    "output_file('frequency.html')\n",
    "show(dot)  # open a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "posts_relevancy = {}\n",
    "relevancy = []\n",
    "count = 0\n",
    "\n",
    "for key,val in posts.items():\n",
    "    relevancy_score = 0\n",
    "    if contains_word(posts[key], query):\n",
    "        if contains_word(posts[key], freq_words[0]):\n",
    "            relevancy_score += 5\n",
    "        if contains_word(posts[key], freq_words[1]):\n",
    "            relevancy_score += 4\n",
    "        if contains_word(posts[key], freq_words[2]):\n",
    "            relevancy_score += 3\n",
    "        if contains_word(posts[key], freq_words[3]):\n",
    "            relevancy_score += 2\n",
    "        if contains_word(posts[key], freq_words[4]):\n",
    "            relevancy_score += 1\n",
    "    relevancy.append(relevancy_score)\n",
    "#     print('Thread ' + str(key) + ' has a relevancy score of ' + str(relevancy_score))\n",
    "    \n",
    "for i in range(len(keys)):\n",
    "    posts_relevancy[keys[i]] = relevancy[i]\n",
    "    \n",
    "print(sorted(relevancy, reverse=True))\n",
    "\n",
    "most_relevant_posts = dict(sorted(posts_relevancy.items(), key=operator.itemgetter(1), reverse=True)[:5])\n",
    "\n",
    "print(most_relevant_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_keys = list(most_relevant_posts.keys())\n",
    "\n",
    "for i in result_keys:\n",
    "    print(posts[i][:500])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
