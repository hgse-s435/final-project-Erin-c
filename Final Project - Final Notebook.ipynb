{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**: describe your dataset, and why you're interested in it\n",
    "\n",
    "* I am using the ScratchEd Forum data. This dataset contains ~1400 threads whereby educators share stories, exchange resources, ask questions, and find people. \n",
    "\n",
    "* I am interested in how people use this tool and how the experience can be enhanced by providing educators with more relevant search results.\n",
    "\n",
    "**Research question(s)**: describe the overall research question of your  project\n",
    "\n",
    "1. What topics are teachers talking about most? \n",
    "  1. How do teachers use the ScratchEd community forums?\n",
    "\n",
    "\n",
    "2. Can data analysis provide better search results for teachers new to the ScratchEd community?\n",
    "\n",
    "\n",
    "##### Hypotheses Prompts:\n",
    "1. Describe 2-3 hypotheses that you're planning to test with your dataset\n",
    "2. Each hypotheses should be based on academic research (cite a paper) and/or background knowledge that you have about the dataset if you've collected it yourself (e.g., if you've conducted interviews)\n",
    "3. Each hypotheses should be formulated as an affirmation (and not a question)\n",
    "4. You can also describe alternative hypotheses, if you think that your results could go either way (but again, have a rationale as for why)\n",
    "\n",
    "##### Hypotheses:\n",
    "1. Teachers will mainly be talking about curriculum and pedagogy\n",
    "  1. Use the community to get more resources and lesson plans from other teachers in the community\n",
    "\n",
    "2. Showing threads with more words that get used frequently with the queried word will yield more relevant results\n",
    "\n",
    "    \n",
    "##### Research:\n",
    "* [Finding Question-Answer Pairs from Online Forums](http://delivery.acm.org/10.1145/1400000/1390415/p467-cong.pdf?ip=140.247.235.144&id=1390415&acc=ACTIVE%20SERVICE&key=AA86BE8B6928DDC7%2EC82FBC3DCC335AD2%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1555078563_4f575e79806487d4099a316d3513da89)\n",
    "* [Using text mining and sentiment analysis for online forums hotspot detection\n",
    "and forecast](https://pdf.sciencedirectassets.com/271653/1-s2.0-S0167923609X00096/1-s2.0-S0167923609002097/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEN7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIDkbiHEu7OmkRpXuQ47n%2B5RYc9qtnHu7nnjYArWjllthAiEAtOUtR6%2Fnf%2BMPpHHBnWAfW4cBYTpk5DC1MKB4KsNGIhIq4wMIpv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDF6pSTqoCur355dVJiq3AzYPgbngPA72q7DgFv9w%2FlWkNFJDvlf7E6thqa2KRPIEz95g%2BWQFJTMYPfbBAIIOZH5cj%2Fg4eLP%2FPix6poFbiiUM1hvEVxq3ZX%2BOEw13pxalLU0iYvvt9STP2U8Y8GJ5GoaIr22sjKQcbsKH6ivusoZf1MjztiuCKSPVxR0IMUiAu4mwVsiFSJNNY4Wmr%2Bv8P9osoVxfX4%2BDJeOPj0aJiBQx%2BnzkdimWgHIoI7Ov3cLHEvtO9SutL%2FSxZwKWgjp81jrmDhARey9NqYTkaJiI53puYu9uoR%2B0jUGZBJpYlmrnQghcfU2FTiT3w0sVnU%2FRs54jTHBHQhG3MNWHsQ%2B8%2BfLmDryJLEtRyANryp5t7IWstINb2ttzPzXHvrwX7rD20vb5jtRn4JB0dm4TenSp9RmJOnvF3%2Byi83V%2BGnKpwYiSnkkZOJEyQN62pBU7zZt6jajObO40NDVRGo5MmcBK3AsQdf7zO%2FLqu3MXaB167vl7qrzVzsTn8wkHNM45TSw2SsqqQogegDDLAfCFBJjPIiiI3thCa33S0qMWBlR8a9WU000MAbIcsfw7rSjuoYx1LsuZAYXJU4cwtZvC5QU6tAEjcPacKzrCJQ3v45AZ29vfEXkrg9nWp2j8EWM4Av9XB602Cpdp1a%2FYYQ%2FfdXj1SilBXvZDJdmraU%2B36auSxfLxWBwxmPS5gcvrVXY3jjMQ6AA7JtcOtag9vu7UXSBhaSzSt4KtnlMM13and2LppDYZuuGnu2PEYkkdkuBN%2Bj%2FrD%2B3ULdKcD2xKEnG7g7vXUbSDRvm5QVKC%2B2QIEZSKEe2WPqL4It9yfhjE8t%2BSWyawaJAJdpg%3D&AWSAccessKeyId=ASIAQ3PHCVTYYVCQWGPK&Expires=1555077390&Signature=IRFyIe%2FyWqhLDQm9C1TEACraFZU%3D&hash=866cdf779d51111c7abe8c62c531189fadf5840a7a84402811c3a1d569dcef3a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167923609002097&tid=spdf-6e8db930-496a-4df4-8bcd-5d24ed7624dc&sid=63bd0fd445fc87442a7ac7b112045e582444gxrqa&type=client)\n",
    "* [Sentiment Analysis and Opinion Mining: A Survey](https://pdfs.semanticscholar.org/261e/26ae134b8f63270dbcacf2d07fa700fdf593.pdf)\n",
    "* [Mapping the research trends by co-word analysis based on keywords from funded project](https://reader.elsevier.com/reader/sd/pii/S1877050916313333?token=4229D8135FA9E0D927EC4BE404B0B4502DF94459FCEC6F3F935DD938E8DFE5D385326E1CC53CB40CD0EEE32D29F9DE30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your raw data below; provide definition / explanations for the measures you're using:\n",
    "\n",
    "The raw data consists of two tables: topics and replies. The topics data is the parent of each conversation thread and the replies data are the children of each topic. Each csv contains the body of the message, a thread ID, and is associated with the topic title, which is essentially like the subject line of an email.\n",
    "\n",
    "I'll be using this data to determine which words are closely related with others in usage by clustering them. I will then use this information to assign relevancy scores to the existing threads to return more relevant results for keyword searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444\n",
      "1444\n",
      "1444\n"
     ]
    }
   ],
   "source": [
    "# massage data into the right format\n",
    "\n",
    "path = '/Users/erincarvalho/Desktop/dev/final-project-Erin-c'\n",
    "if os.path.isdir(path + '/txt_files'):\n",
    "    shutil.rmtree(path + '/txt_files', ignore_errors=False, onerror=None)\n",
    "os.mkdir(path + '/txt_files')\n",
    "\n",
    "# creates a separate text file for each topic with all posts and replies from csv\n",
    "\n",
    "ids = []\n",
    "\n",
    "with open('ScratchEd_all_data.csv', \"r\", encoding='utf-8', errors='ignore') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    count = 0\n",
    "\n",
    "    for idx, row in enumerate(csv_reader):   \n",
    "        if str(row[0]) in ids:\n",
    "            filename = path + '/txt_files/topic_' + str(row[0]) + '.txt'\n",
    "            file = open(filename,'a+')\n",
    "            contents = str(row[3]) + '\\r\\n' + '\\r\\n'\n",
    "            file.write(contents)\n",
    "        else:\n",
    "            filename = path + '/txt_files/topic_' + str(row[0]) + '.txt'\n",
    "            file = open(filename,'a+')\n",
    "            contents = str(row[3]) + '\\r\\n' + '\\r\\n'\n",
    "            file.write(contents)\n",
    "            ids.append(str(row[0]))\n",
    "            count += 1\n",
    "    \n",
    "threads = glob.glob('./txt_files/*.txt')\n",
    "\n",
    "print(count)\n",
    "print(len(ids))\n",
    "print(len(threads))\n",
    "\n",
    "# load actual text into a list\n",
    "\n",
    "documents = []\n",
    "\n",
    "for thread in threads: \n",
    "    with open (thread, \"r\", encoding='utf-8', errors='ignore') as t:\n",
    "        documents.append(t.read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cleans a list of documents \n",
    "\n",
    "def clean_documents(documents):\n",
    "\n",
    "    cleaned_docs = []\n",
    "\n",
    "    punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "              'very', 's', 't', 'can', 'will', 'just', 'don', 'should', \n",
    "              'now', 'http', 'https', 'edu', 'www', 'com', 'scratch', \n",
    "              'mit', 'org', 'would', 'should', 'could', 'might', 'really', \n",
    "              'very', 'good', 'great', 'best', 'karen', '྾explore',\n",
    "              '྾interact', '྾network', 'get', 'also', 'let', 'much', 'use', \n",
    "              'les', 'ver', 'post', 'est', 'oscar', 'con', 'las', 'para',\n",
    "              'student', 'projects', 'january', 'february', 'march', 'april',\n",
    "              'may', 'june', 'july', 'august', 'september', 'october', \n",
    "              'november','december', 'monday', 'tuesday', 'wednesday', 'thursday',\n",
    "              'friday', 'saturday', 'sunday']\n",
    "\n",
    "    for i,doc in enumerate(documents):\n",
    "        # removes new lines and carriage returns\n",
    "        doc = doc.replace('\\n', ' ')\n",
    "        doc = doc.replace('\\r', ' ')\n",
    "        # remove ponctuation\n",
    "        for punc in punctuation: \n",
    "            doc = doc.replace(punc, ' ')\n",
    "        # remove numbers\n",
    "        for i in range(10):\n",
    "            doc = doc.replace(str(i), ' ')\n",
    "        # remove stop words as the first word in a string\n",
    "        if doc.split(' ', 1)[0] in stop_words:\n",
    "            doc = doc.replace(doc.split(' ', 1)[0] + ' ', ' ')\n",
    "        # remove stop words everywhere else\n",
    "        for stop_word in stop_words:\n",
    "            doc = doc.replace(' ' + stop_word + ' ', ' ')\n",
    "        # remove single characters and stem the words \n",
    "        doc = [x for x in doc.split() if len(x) > 2]\n",
    "        doc = \" \".join(doc)\n",
    "        # save the result to our list of documents\n",
    "        cleaned_docs.append(doc)\n",
    "        \n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = clean_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nltk\n",
    "# !nltk.download(\"wordnet\", \"./\")\n",
    "\n",
    "# import necessary libraries\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a lemmatized vocabulary\n",
    "\n",
    "def get_vocabulary(documents):\n",
    "\n",
    "    lemmatized_vocabulary = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "\n",
    "    for document in documents:\n",
    "        tokens = word_tokenize(document)\n",
    "        for word, tag in pos_tag(tokens):\n",
    "            word = lemmatizer.lemmatize(word, pos=tag_map[tag[0]])\n",
    "            if wn.synsets(word):\n",
    "                if word not in lemmatized_vocabulary: \n",
    "                    lemmatized_vocabulary.append(word)\n",
    "\n",
    "    lemmatized_vocabulary = list(set(lemmatized_vocabulary))\n",
    "    lemmatized_vocabulary.sort()\n",
    "\n",
    "    return lemmatized_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7688\n"
     ]
    }
   ],
   "source": [
    "vocabulary = get_vocabulary(documents)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds a Pandas DataFrame with normalized frequency counts by document\n",
    "\n",
    "def preprocess_data(documents, window_size=100, overlap=25):\n",
    "    \n",
    "    # flatten all text to one document\n",
    "\n",
    "    new_list_of_documents = []\n",
    "    \n",
    "    # flatten everything into one string\n",
    "    \n",
    "    flat = \"\"\n",
    "    for document in documents:\n",
    "        flat += document\n",
    "    \n",
    "    # split into words\n",
    "    \n",
    "    flat = flat.split()\n",
    "\n",
    "    # create chunks of 100 words\n",
    "    \n",
    "    high = window_size\n",
    "    while high < len(flat):\n",
    "        low = high - window_size\n",
    "        new_list_of_documents.append(flat[low:high])\n",
    "        high += overlap\n",
    "        \n",
    "    chunks = new_list_of_documents\n",
    "\n",
    "    # build a data frame of vocabulary counts per document\n",
    "    \n",
    "    df = pd.DataFrame(0, index=np.arange(len(chunks)), columns=vocabulary)\n",
    "    \n",
    "    # fill out the matrix with counts\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        for word in chunk:\n",
    "            if word in df.columns: \n",
    "                df.loc[i,word] += 1\n",
    "                \n",
    "    # weight words based on Sherin's paper and normalize\n",
    "    \n",
    "    def one_plus_log(cell):\n",
    "        if cell != 0: \n",
    "            return 1 + math.log(cell)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df = df.applymap(one_plus_log)\n",
    "    \n",
    "    scaler = Normalizer()\n",
    "    \n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    \n",
    "    # transform deviation vectors\n",
    "    \n",
    "    v_sum = np.sum(df.values, axis=0)\n",
    "\n",
    "    v_avg = v_sum / np.sqrt(np.dot(v_sum, v_sum))\n",
    "    \n",
    "    matrix = df.values\n",
    "\n",
    "    for row in range(df.shape[0]):\n",
    "\n",
    "        # this is one vector (row)\n",
    "        \n",
    "        v_i = matrix[row,:]\n",
    "\n",
    "        # we subtract its component along v_average\n",
    "        \n",
    "        scalar = np.dot(v_i,v_avg)\n",
    "        sub = v_avg * scalar\n",
    "\n",
    "        # we replace the row by the deviation vector\n",
    "        \n",
    "        matrix[row,:] = (v_i - sub) / (np.sqrt(np.dot(v_i - sub, v_i - sub)))\n",
    "        \n",
    "    return [df,chunks]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>aba</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbot</th>\n",
       "      <th>abbreviate</th>\n",
       "      <th>abel</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abo</th>\n",
       "      <th>abound</th>\n",
       "      <th>...</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zany</th>\n",
       "      <th>zaragoza</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhou</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000890</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>-0.002879</td>\n",
       "      <td>-0.014247</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>-0.000283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.003279</td>\n",
       "      <td>-0.016224</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000401</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.000322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001117</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>-0.017876</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000620</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>-0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001118</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.003616</td>\n",
       "      <td>-0.017890</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000621</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>-0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001038</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>-0.016625</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000577</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000491</td>\n",
       "      <td>-0.000330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7688 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaron       aba  abandon     abbot  abbreviate      abel   ability  \\\n",
       "0 -0.000890 -0.000043      0.0 -0.000079         0.0 -0.000085 -0.002879   \n",
       "1 -0.001013 -0.000049      0.0 -0.000090         0.0 -0.000097 -0.003279   \n",
       "2 -0.001117 -0.000054      0.0 -0.000099         0.0 -0.000107 -0.003613   \n",
       "3 -0.001118 -0.000054      0.0 -0.000099         0.0 -0.000107 -0.003616   \n",
       "4 -0.001038 -0.000050      0.0 -0.000092         0.0 -0.000100 -0.003360   \n",
       "\n",
       "       able       abo    abound  ...     yukon     yummy      zany  zaragoza  \\\n",
       "0 -0.014247 -0.000010 -0.000043  ... -0.000131 -0.000044 -0.000043 -0.000092   \n",
       "1 -0.016224 -0.000012 -0.000049  ... -0.000149 -0.000050 -0.000049 -0.000105   \n",
       "2 -0.017876 -0.000013 -0.000054  ... -0.000165 -0.000055 -0.000054 -0.000116   \n",
       "3 -0.017890 -0.000013 -0.000054  ... -0.000165 -0.000055 -0.000054 -0.000116   \n",
       "4 -0.016625 -0.000012 -0.000050  ... -0.000153 -0.000051 -0.000050 -0.000108   \n",
       "\n",
       "       zero      zhou       zip    zombie      zone      zoom  \n",
       "0 -0.000352 -0.000043 -0.000494 -0.000070 -0.000421 -0.000283  \n",
       "1 -0.000401 -0.000049 -0.000563 -0.000080 -0.000479 -0.000322  \n",
       "2 -0.000442 -0.000054 -0.000620 -0.000088 -0.000528 -0.000355  \n",
       "3 -0.000443 -0.000054 -0.000621 -0.000088 -0.000528 -0.000355  \n",
       "4 -0.000411 -0.000050 -0.000577 -0.000082 -0.000491 -0.000330  \n",
       "\n",
       "[5 rows x 7688 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = preprocess_data(documents)[0]\n",
    "\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import collections\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get clusters of words using kmeans and agglomerative clustering and nearest centroid\n",
    "\n",
    "def kmeans_clusters(df, n_clusters = 10, n_words = 10):\n",
    "    kmeans_obj = KMeans(n_clusters, max_iter = 1000).fit(df.values)\n",
    "\n",
    "    top_words = collections.defaultdict(lambda: [])\n",
    "\n",
    "    # iterate through each cluster\n",
    "    for n in range(kmeans_obj.n_clusters):\n",
    "        \n",
    "        print('CLUSTER ' + str(n + 1) + ': ', end='')\n",
    "\n",
    "        # get the cluster centers\n",
    "        arr = kmeans_obj.cluster_centers_[n]\n",
    "\n",
    "        # sorts the array and keep the last n words\n",
    "        indices = arr.argsort()[-n_words:]\n",
    "\n",
    "        # add the words to the list of words\n",
    "        for i in indices:\n",
    "            print(vocabulary[i], end=', ')\n",
    "            top_words[n].append(vocabulary[i])\n",
    "        \n",
    "        print('')\n",
    "        \n",
    "    return top_words\n",
    "\n",
    "def agglomerative_clusters(df, n_clusters = 10, linkage='ward'):\n",
    "    ward = AgglomerativeClustering(n_clusters, linkage=linkage).fit(df.values)\n",
    "    label = ward.labels_\n",
    "    return label\n",
    "    \n",
    "def centroid_clusters(df, n_clusters, centroids, n_words=10, printed=True):   \n",
    "    # try to get the most informative words of each cluster\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(df.values, agglomerative_clusters(df_preprocessed))\n",
    "    words = {}\n",
    "    vocabulary = df.columns\n",
    "    for n in range(n_clusters):\n",
    "        words[n] = []\n",
    "        if printed: print('CLUSTER ' + str(n+1) + ': ', end='')\n",
    "        arr = centroids[n]\n",
    "        indices = arr.argsort()[-n_words:]\n",
    "        for i in indices:\n",
    "            if printed: print(vocabulary[i], end=', '),\n",
    "            words[n].append(vocabulary[i])\n",
    "        print('')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans Clusters\n",
      "CLUSTER 1: blog, status, photo, like, twitter, found, share, please, news, week, \n",
      "CLUSTER 2: think, need, try, work, love, version, thank, like, know, thanks, \n",
      "CLUSTER 3: curriculum, teaching, teachers, technology, teach, science, learning, school, computer, programming, \n",
      "CLUSTER 4: address, studio, created, password, classroom, email, create, class, teacher, account, \n",
      "CLUSTER 5: creative, new, guide, computing, workshop, day, educator, educators, join, scratched, \n",
      "CLUSTER 6: class, see, used, sprites, work, create, different, make, one, project, \n",
      "CLUSTER 7: strategies, core, lab, underlying, together, previous, share, practice, ideas, educators, \n",
      "CLUSTER 8: way, code, right, make, script, sprites, one, blocks, block, sprite, \n",
      "CLUSTER 9: rosemary, attendees, led, shared, people, networking, recap, group, session, breakout, \n",
      "CLUSTER 10: related, team, throughout, news, weekly, stories, place, week, scratched, roundup, \n",
      "Agglomerative Clusters\n",
      "[9 9 9 ... 0 0 0]\n",
      "CLF Clusters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'visualize_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ead1cecab0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CLF Clusters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclf_top_words_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualize_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_preprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroids_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroids_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize_clusters' is not defined"
     ]
    }
   ],
   "source": [
    "print('KMeans Clusters')\n",
    "kmeans_top_word_clusters = kmeans_clusters(df_preprocessed)\n",
    "\n",
    "print('Agglomerative Clusters')\n",
    "agglomerative_top_word_clusters = print(agglomerative_clusters(df_preprocessed))\n",
    "                                                         \n",
    "print('CLF Clusters')\n",
    "clf_top_words_clusters = visualize_clusters(df_preprocessed, clf.centroids_.shape[0], clf.centroids_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clusters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "from bokeh.palettes import Category10\n",
    "from IPython.core.display import HTML\n",
    "from bokeh.plotting import ColumnDataSource, figure, show, output_file\n",
    "from bokeh.io import output_notebook, curdoc\n",
    "from bokeh.models import HoverTool, Select, Slider\n",
    "from bokeh.layouts import row, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = Category10[10]\n",
    "html_text = \"\"\n",
    "kmeans_obj = KMeans(10, max_iter = 1000).fit(df_preprocessed.values)\n",
    "\n",
    "\n",
    "for i in range(0,kmeans_obj.n_clusters):\n",
    "    words=', '.join(top_words[i])\n",
    "    color = colors[i]\n",
    "    num = str(i)\n",
    "    text = \"<p>Cluster \"+num+\": <font color='\"+color+\"'>\"+words+\"</font></p>\"\n",
    "    html_text += text\n",
    "\n",
    "HTML(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(documents)[1]\n",
    "indices =  list(range(0, len(chunks)))\n",
    "list_of_chunks = [' '.join(chunks[i]) for i in indices]\n",
    "labels = [kmeans_obj.labels_[i] for i in indices]\n",
    "palette = [colors[labels[i]] for i in indices]\n",
    "doc_id = []\n",
    "current_doc = 0\n",
    "next_doc = 1\n",
    "\n",
    "# we go through all the chunks \n",
    "for chunk in list_of_chunks:\n",
    "    next_doc = current_doc + 1\n",
    "    if next_doc == len(clean_docs):\n",
    "        doc_id.append(current_doc)\n",
    "    else:\n",
    "        if chunk in clean_docs[next_doc]:\n",
    "            current_doc += 1\n",
    "        doc_id.append(current_doc)\n",
    "        \n",
    "master = {'indices': indices,\n",
    "          'chunk': list_of_chunks, \n",
    "          'cluster': labels,\n",
    "          'document': doc_id, \n",
    "          'palette': palette }\n",
    "\n",
    "master_df = pd.DataFrame.from_dict(master)\n",
    "\n",
    "master_df.head(10)\n",
    "\n",
    "source = ColumnDataSource(master_df)\n",
    "\n",
    "# Create a figure with the \"box_select\" tool: p\n",
    "p = figure(tools='box_select',x_axis_label='indices',y_axis_label='document')\n",
    "\n",
    "# Add circle glyphs to the figure p\n",
    "p.circle('indices','document', source=source, color='green', size=8)\n",
    "\n",
    "# Specify the name of the output file and show the result\n",
    "output_file('clusters.html')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
